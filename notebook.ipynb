{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.feature_extraction.stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstop_words\u001b[39;00m \u001b[39mimport\u001b[39;00m ENGLISH_STOP_WORDS\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.feature_extraction.stop_words'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing Text\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "\n",
    "# nltk module\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/khoimai/Documents/vrn/programmazione/plotly-nlp-news'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_df = pd.read_csv(\"data/raw-data.csv\")\n",
    "orginal_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "  \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "\n",
    "  # specific\n",
    "  phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "  phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "  phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "  # general\n",
    "  phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "  phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "  phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "  phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "  phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "  phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "  phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "  return phrase\n",
    "\n",
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "# def remove_number(word):\n",
    "#     result = re.sub(r'\\d+', '', word)\n",
    "#     return result\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "def replace_newline(word):\n",
    "    return word.replace('\\n','')\n",
    "\n",
    "def remove_html(word):\n",
    "  \"\"\"remove_html takes raw text and removes html tags from the text.\n",
    "     ref: stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "     \"\"\"\n",
    "  result = BeautifulSoup(word, 'lxml')\n",
    "  return result.get_text()\n",
    "\n",
    "def remove_special_character(phrase, remove_number=False):\n",
    "  \"\"\"remove_special_character takes text and removes special charcters.\n",
    "     ref: https://stackoverflow.com/a/18082370/4084039\"\"\"\n",
    "\n",
    "  phrase = re.sub(\"\\S*\\d\\S*\", \"\", phrase).strip()\n",
    "  if remove_number:\n",
    "    phrase = re.sub('[^A-Za-z]+', ' ', phrase)\n",
    "  else:\n",
    "    phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase)\n",
    "  return phrase\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result\n",
    "\n",
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_utils = [remove_html,\n",
    "                      remove_hyperlink,\n",
    "                      replace_newline,\n",
    "                      #remove_number,\n",
    "                      decontracted,\n",
    "                      remove_special_character,\n",
    "                      remove_punctuation,\n",
    "                      remove_whitespace,\n",
    "                      #remove_stop_words,\n",
    "                      #word_lemmatizer\n",
    "                      ]\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug 11 2023, 19:44:49) \n[Clang 15.0.0 (clang-1500.0.40.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63d73fe1043587723a932559fd3ea11ed24d928b7ad450cb3ce16715aed1b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
