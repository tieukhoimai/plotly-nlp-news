{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>words_clipped</th>\n",
       "      <th>words_clipped_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>million american roll sleev omicrontarget covi...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>health expert said earli predict demand match ...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>funniest tweet cat dog week sept</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>dog understand eaten</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>funniest tweet parent week sept</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>accident grownup toothpast toddler toothbrush ...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>american airlin flyer charg ban life punch fli...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>subdu passeng crew fled aircraft confront acco...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>woman call cop black birdwatch lose lawsuit ex...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>ami cooper accus invest firm franklin templeto...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  million american roll sleev omicrontarget covi...  U.S. NEWS   \n",
       "2                   funniest tweet cat dog week sept     COMEDY   \n",
       "3                    funniest tweet parent week sept  PARENTING   \n",
       "1  american airlin flyer charg ban life punch fli...  U.S. NEWS   \n",
       "4  woman call cop black birdwatch lose lawsuit ex...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  health expert said earli predict demand match ...  Carla K. Johnson, AP   \n",
       "2                               dog understand eaten         Elyse Wanshel   \n",
       "3  accident grownup toothpast toddler toothbrush ...      Caroline Bologna   \n",
       "1  subdu passeng crew fled aircraft confront acco...        Mary Papenfuss   \n",
       "4  ami cooper accus invest firm franklin templeto...        Nina Golgowski   \n",
       "\n",
       "         date  words_clipped  words_clipped_headline  \n",
       "0  2022-09-23             29                      11  \n",
       "2  2022-09-23             12                      13  \n",
       "3  2022-09-23             25                       9  \n",
       "1  2022-09-23             28                      13  \n",
       "4  2022-09-22             25                      11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/processed_data.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization again\n",
    "\n",
    "data = list(df['headline'].values)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]]\n",
      "[[('american', 1), ('booster', 1), ('covid', 1), ('million', 1), ('omicrontarget', 1), ('roll', 1), ('sleev', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           passes=5,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"model/model_lda_100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel.load(\"model/model_lda_100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"travel\" + 0.024*\"poll\" + 0.018*\"mother\" + 0.017*\"spring\" + '\n",
      "  '0.017*\"famili\" + 0.013*\"man\" + 0.012*\"school\" + 0.012*\"right\" + '\n",
      "  '0.012*\"black\" + 0.011*\"nation\"'),\n",
      " (1,\n",
      "  '0.043*\"way\" + 0.041*\"look\" + 0.037*\"home\" + 0.028*\"tip\" + 0.023*\"like\" + '\n",
      "  '0.021*\"babi\" + 0.020*\"need\" + 0.017*\"know\" + 0.014*\"model\" + 0.013*\"hair\"'),\n",
      " (2,\n",
      "  '0.237*\"photo\" + 0.096*\"video\" + 0.022*\"recip\" + 0.020*\"dress\" + 0.017*\"get\" '\n",
      "  '+ 0.013*\"eat\" + 0.008*\"evolut\" + 0.008*\"realli\" + 0.008*\"perfect\" + '\n",
      "  '0.007*\"break\"'),\n",
      " (3,\n",
      "  '0.033*\"marriag\" + 0.023*\"good\" + 0.019*\"guid\" + 0.018*\"obama\" + 0.015*\"dad\" '\n",
      "  '+ 0.014*\"valentin\" + 0.013*\"step\" + 0.012*\"come\" + 0.011*\"fear\" + '\n",
      "  '0.011*\"watch\"'),\n",
      " (4,\n",
      "  '0.035*\"studi\" + 0.034*\"fashion\" + 0.027*\"child\" + 0.021*\"find\" + '\n",
      "  '0.018*\"talk\" + 0.017*\"cancer\" + 0.017*\"design\" + 0.016*\"fall\" + '\n",
      "  '0.015*\"show\" + 0.015*\"weight\"'),\n",
      " (5,\n",
      "  '0.040*\"love\" + 0.022*\"live\" + 0.021*\"beauti\" + 0.020*\"star\" + 0.015*\"super\" '\n",
      "  '+ 0.015*\"heart\" + 0.014*\"noth\" + 0.014*\"bowl\" + 0.013*\"kate\" + '\n",
      "  '0.013*\"secret\"'),\n",
      " (6,\n",
      "  '0.065*\"best\" + 0.060*\"make\" + 0.014*\"go\" + 0.012*\"bad\" + 0.012*\"let\" + '\n",
      "  '0.011*\"bank\" + 0.011*\"award\" + 0.010*\"street\" + 0.010*\"relationship\" + '\n",
      "  '0.009*\"stay\"'),\n",
      " (7,\n",
      "  '0.065*\"day\" + 0.055*\"new\" + 0.038*\"week\" + 0.027*\"kid\" + 0.023*\"life\" + '\n",
      "  '0.023*\"world\" + 0.022*\"parent\" + 0.022*\"food\" + 0.018*\"celebr\" + '\n",
      "  '0.018*\"year\"'),\n",
      " (8,\n",
      "  '0.043*\"divorc\" + 0.027*\"say\" + 0.025*\"woman\" + 0.018*\"summer\" + '\n",
      "  '0.017*\"chang\" + 0.016*\"america\" + 0.016*\"doe\" + 0.015*\"want\" + 0.015*\"girl\" '\n",
      "  '+ 0.015*\"american\"'),\n",
      " (9,\n",
      "  '0.051*\"wed\" + 0.040*\"style\" + 0.026*\"health\" + 0.017*\"idea\" + 0.016*\"hotel\" '\n",
      "  '+ 0.013*\"coupl\" + 0.012*\"reason\" + 0.011*\"big\" + 0.011*\"plan\" + '\n",
      "  '0.011*\"money\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 row: ([(0, 0.18569507), (1, 0.07249465), (2, 0.082396485), (3, 0.11860351), (4, 0.07902971), (5, 0.06865962), (6, 0.07279318), (7, 0.098926), (8, 0.1464891), (9, 0.074912645)], [(0, [8]), (1, [0]), (2, [9]), (3, [0]), (4, []), (5, [3]), (6, [])], [(0, [(8, 0.9999357)]), (1, [(0, 0.9946801)]), (2, [(9, 0.02032671)]), (3, [(0, 0.99991685)]), (4, []), (5, [(3, 0.99953455)]), (6, [])])\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(lda_model[corpus]):\n",
    "    print(i,\"row:\", row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39msorted\u001b[39;49m(row, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: (x[\u001b[39m1\u001b[39;49m]), reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "sorted(row, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts, dates):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  \n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = [\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\"]\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, pd.Series(dates)], axis=1)\n",
    "    return sent_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "\n",
    "# Get main topic in each document\n",
    "for i, row in enumerate(lda_model[corpus]):\n",
    "    row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "    # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        if j == 0:  \n",
    "            wp = lda_model.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            sent_topics_df = sent_topics_df._append(\n",
    "                pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Add original text to the end of the output\n",
    "contents = pd.Series(data)\n",
    "\n",
    "sent_topics_df = pd.concat([sent_topics_df, contents, pd.Series(list(df[\"date\"].values))], axis=1)\n",
    "\n",
    "sent_topics_df.columns = [\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\", \"headline\", \"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>travel, poll, mother, spring, famili, man, sch...</td>\n",
       "      <td>million american roll sleev omicrontarget covi...</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.3813</td>\n",
       "      <td>day, new, week, kid, life, world, parent, food...</td>\n",
       "      <td>funniest tweet cat dog week sept</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>day, new, week, kid, life, world, parent, food...</td>\n",
       "      <td>funniest tweet parent week sept</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>travel, poll, mother, spring, famili, man, sch...</td>\n",
       "      <td>american airlin flyer charg ban life punch fli...</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>travel, poll, mother, spring, famili, man, sch...</td>\n",
       "      <td>woman call cop black birdwatch lose lawsuit ex...</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187017</th>\n",
       "      <td>2</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>photo, video, recip, dress, get, eat, evolut, ...</td>\n",
       "      <td>daili correspond clip week al madrig biggest m...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187018</th>\n",
       "      <td>9</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>wed, style, health, idea, hotel, coupl, reason...</td>\n",
       "      <td>mitt romney mad florida edit video</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187019</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>photo, video, recip, dress, get, eat, evolut, ...</td>\n",
       "      <td>amaz generat photo</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187020</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>travel, poll, mother, spring, famili, man, sch...</td>\n",
       "      <td>russian cargo ship dock intern space station</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187021</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>photo, video, recip, dress, get, eat, evolut, ...</td>\n",
       "      <td>dwight howard rip teammat magic loss hornet</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187022 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic  Perc_Contribution  \\\n",
       "0                    0             0.1857   \n",
       "1                    7             0.3813   \n",
       "2                    7             0.3496   \n",
       "3                    0             0.2628   \n",
       "4                    0             0.2166   \n",
       "...                ...                ...   \n",
       "187017               2             0.2253   \n",
       "187018               9             0.1640   \n",
       "187019               2             0.1442   \n",
       "187020               0             0.2993   \n",
       "187021               2             0.1292   \n",
       "\n",
       "                                           Topic_Keywords  \\\n",
       "0       travel, poll, mother, spring, famili, man, sch...   \n",
       "1       day, new, week, kid, life, world, parent, food...   \n",
       "2       day, new, week, kid, life, world, parent, food...   \n",
       "3       travel, poll, mother, spring, famili, man, sch...   \n",
       "4       travel, poll, mother, spring, famili, man, sch...   \n",
       "...                                                   ...   \n",
       "187017  photo, video, recip, dress, get, eat, evolut, ...   \n",
       "187018  wed, style, health, idea, hotel, coupl, reason...   \n",
       "187019  photo, video, recip, dress, get, eat, evolut, ...   \n",
       "187020  travel, poll, mother, spring, famili, man, sch...   \n",
       "187021  photo, video, recip, dress, get, eat, evolut, ...   \n",
       "\n",
       "                                                 headline        date  \n",
       "0       million american roll sleev omicrontarget covi...  2022-09-23  \n",
       "1                        funniest tweet cat dog week sept  2022-09-23  \n",
       "2                         funniest tweet parent week sept  2022-09-23  \n",
       "3       american airlin flyer charg ban life punch fli...  2022-09-23  \n",
       "4       woman call cop black birdwatch lose lawsuit ex...  2022-09-22  \n",
       "...                                                   ...         ...  \n",
       "187017  daili correspond clip week al madrig biggest m...  2012-01-28  \n",
       "187018                 mitt romney mad florida edit video  2012-01-28  \n",
       "187019                                 amaz generat photo  2012-01-28  \n",
       "187020       russian cargo ship dock intern space station  2012-01-28  \n",
       "187021        dwight howard rip teammat magic loss hornet  2012-01-28  \n",
       "\n",
       "[187022 rows x 5 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.to_csv('data/lda_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63d73fe1043587723a932559fd3ea11ed24d928b7ad450cb3ce16715aed1b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
